{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ZIA Threat Hunting Lab — Search Engine Queries\n\nThis notebook:\n1. Loads `../data/lab_dprk.csv`\n2. Extracts **host**, **registrable domain**, and **TLD** from the `URL` field\n3. Filters to **search-engine** traffic (Google/Bing/Yahoo/DuckDuckGo/Yandex/etc.)\n4. Carves out the **search terms** from the URL querystring\n5. Makes search terms **searchable** (interactive UI)\n6. Flags any search terms matching a **dirty wordlist** from `../data/dirty_wordlist.txt` (case-insensitive)\n\n> Tip: This is a *behavioral* lens. You’re looking for potentially risky intent, not making accusations.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- Imports ---\nimport re\nimport math\nimport pandas as pd\n\nfrom urllib.parse import urlsplit, parse_qs, unquote_plus\n\nimport ipywidgets as widgets\nimport plotly.express as px\nfrom IPython.display import display, HTML\n\n# Optional: PSL-aware registrable-domain extraction\n# If missing, install once:  !pip -q install tldextract\ntry:\n    import tldextract\n    _TLDEXTRACT = tldextract.TLDExtract(cache_dir=False)\nexcept Exception:\n    _TLDEXTRACT = None\n    print(\"WARNING: tldextract not available. Falling back to naive domain parsing.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "_SCHEME_RE = re.compile(r\"^[a-zA-Z][a-zA-Z0-9+.-]*://\")\n\ndef _normalize_url_for_parsing(u: str) -> str:\n    \"\"\"Ensure the URL has a scheme so urlsplit can reliably find hostname.\"\"\"\n    if u is None or (isinstance(u, float) and math.isnan(u)):\n        return \"\"\n    u = str(u).strip().strip('\"').strip(\"'\")\n    if not u:\n        return \"\"\n    if not _SCHEME_RE.match(u):\n        u = \"https://\" + u\n    return u\n\ndef extract_host_domain_tld(raw_url: str) -> tuple[str, str, str]:\n    \"\"\"\n    Returns:\n      url_host: full host (incl. subdomain)\n      domain: registrable domain (e.g., cloudflare-dns.com)\n      tld: public suffix (e.g., com, co.uk)\n    \"\"\"\n    u = _normalize_url_for_parsing(raw_url)\n    if not u:\n        return (\"\", \"\", \"\")\n    try:\n        parts = urlsplit(u)\n        host = (parts.hostname or \"\").lower().rstrip(\".\")\n    except Exception:\n        return (\"\", \"\", \"\")\n    if not host:\n        return (\"\", \"\", \"\")\n\n    if _TLDEXTRACT is not None:\n        ext = _TLDEXTRACT(host)\n        tld = (ext.suffix or \"\").lower()\n        if ext.domain and ext.suffix:\n            domain = f\"{ext.domain}.{ext.suffix}\".lower()\n        elif ext.domain:\n            domain = ext.domain.lower()\n        else:\n            domain = host\n        return (host, domain, tld)\n\n    # Naive fallback (less accurate for multi-part suffixes)\n    labels = host.split(\".\")\n    if len(labels) >= 2:\n        tld = labels[-1].lower()\n        domain = \".\".join(labels[-2:]).lower()\n    else:\n        tld = \"\"\n        domain = host\n    return (host, domain, tld)\n\ndef safe_str(x) -> str:\n    return \"\" if x is None or (isinstance(x, float) and math.isnan(x)) else str(x)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "LAB_PATH = \"../data/lab_dprk.csv\"\nDIRTY_PATH = \"../data/dirty_wordlist.txt\"\n\n# Read CSV safely (avoid mixed-type warnings)\ndf = pd.read_csv(LAB_PATH, low_memory=False)\ndf.columns = df.columns.str.strip()\n\n# Parse URL -> host/domain/tld\nparsed = df[\"URL\"].apply(extract_host_domain_tld)\ndf[[\"url_host\", \"domain\", \"tld\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n\ndisplay(df[[\"URL\",\"url_host\",\"domain\",\"tld\"]].head(5))\nprint(\"Rows:\", len(df))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------------------\n# Search engine identification\n# -----------------------------\n# We use registrable domain to match families (e.g., google.com, google.co.uk all map to google.<tld> as domain).\n# If your dataset includes many country Google domains, you can expand patterns.\n\nSEARCH_ENGINES = {\n    # key: engine_name, value: dict(domain_match=set/patterns, query_params=preferred order)\n    \"Google\": {\n        \"domain_allow\": {\"google.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)google\\.[a-z.]+$\", re.I),  # handles google.co.uk if you have PSL extraction\n        \"query_params\": [\"q\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n    \"Bing\": {\n        \"domain_allow\": {\"bing.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)bing\\.com$\", re.I),\n        \"query_params\": [\"q\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n    \"Yahoo\": {\n        \"domain_allow\": {\"search.yahoo.com\", \"yahoo.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)search\\.yahoo\\.com$|(^|\\.)yahoo\\.com$\", re.I),\n        \"query_params\": [\"p\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n    \"DuckDuckGo\": {\n        \"domain_allow\": {\"duckduckgo.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)duckduckgo\\.com$\", re.I),\n        \"query_params\": [\"q\"],\n        \"path_regex\": re.compile(r\"^/\\??$\", re.I),  # DDG often uses /\n    },\n    \"Yandex\": {\n        \"domain_allow\": {\"yandex.com\", \"yandex.ru\"},\n        \"host_regex\": re.compile(r\"(^|\\.)yandex\\.[a-z.]+$\", re.I),\n        \"query_params\": [\"text\", \"query\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n    \"Brave\": {\n        \"domain_allow\": {\"search.brave.com\", \"brave.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)search\\.brave\\.com$\", re.I),\n        \"query_params\": [\"q\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n    \"Startpage\": {\n        \"domain_allow\": {\"startpage.com\"},\n        \"host_regex\": re.compile(r\"(^|\\.)startpage\\.com$\", re.I),\n        \"query_params\": [\"query\", \"q\"],\n        \"path_regex\": re.compile(r\"^/do/search|^/sp/search\", re.I),\n    },\n    \"Ecosia\": {\n        \"domain_allow\": {\"ecosia.org\"},\n        \"host_regex\": re.compile(r\"(^|\\.)ecosia\\.org$\", re.I),\n        \"query_params\": [\"q\"],\n        \"path_regex\": re.compile(r\"^/search\", re.I),\n    },\n}\n\ndef identify_search_engine(url_host: str, domain: str, path: str) -> str:\n    host = (url_host or \"\").lower()\n    dom = (domain or \"\").lower()\n    pth = safe_str(path)\n    for engine, spec in SEARCH_ENGINES.items():\n        # Match by host regex first (most robust across country Google/Yandex, etc.)\n        if spec.get(\"host_regex\") and spec[\"host_regex\"].search(host):\n            # Optionally require a search-ish path\n            pr = spec.get(\"path_regex\")\n            if pr is None or pr.search(pth):\n                return engine\n        # Fallback to known domains\n        if dom in spec.get(\"domain_allow\", set()):\n            pr = spec.get(\"path_regex\")\n            if pr is None or pr.search(pth):\n                return engine\n    return \"\"\n\ndef extract_search_term(raw_url: str, engine: str) -> str:\n    \"\"\"Extract the search term from the URL based on engine-specific query params.\"\"\"\n    if not engine:\n        return \"\"\n    u = _normalize_url_for_parsing(raw_url)\n    if not u:\n        return \"\"\n    parts = urlsplit(u)\n    qs = parse_qs(parts.query, keep_blank_values=True)\n\n    params = SEARCH_ENGINES.get(engine, {}).get(\"query_params\", [\"q\"])\n    for p in params:\n        if p in qs and len(qs[p]) > 0:\n            # parse_qs already decodes percent-encoding and '+' as space for querystring,\n            # but we defensively run unquote_plus in case the field is pre-escaped.\n            term = safe_str(qs[p][0])\n            term = unquote_plus(term)\n            return term.strip()\n    return \"\"\n\n# Build a search-only dataframe\nu = df[\"URL\"].apply(_normalize_url_for_parsing).apply(urlsplit)\ndf[\"_path\"] = u.apply(lambda x: x.path)\ndf[\"search_engine\"] = df.apply(lambda r: identify_search_engine(r[\"url_host\"], r[\"domain\"], r[\"_path\"]), axis=1)\ndf[\"search_term\"] = df.apply(lambda r: extract_search_term(r[\"URL\"], r[\"search_engine\"]), axis=1)\n\nsearch_df = df[(df[\"search_engine\"] != \"\") & (df[\"search_term\"].fillna(\"\").str.len() > 0)].copy()\n\ndisplay(search_df[[\"Logged Time\",\"User\",\"URL\",\"search_engine\",\"search_term\",\"domain\"]].head(10))\nprint(\"Search events:\", len(search_df))\nprint(\"Engines found:\", search_df[\"search_engine\"].value_counts().to_dict())\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------------------\n# Dirty wordlist matching\n# -----------------------------\ndef load_dirty_wordlist(path: str) -> list[str]:\n    words = []\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        for line in f:\n            s = line.strip()\n            if not s:\n                continue\n            if s.startswith(\"#\"):\n                continue\n            words.append(s.lower())\n    # De-dupe, preserve order\n    seen = set()\n    out = []\n    for w in words:\n        if w not in seen:\n            out.append(w)\n            seen.add(w)\n    return out\n\ndirty_words = load_dirty_wordlist(DIRTY_PATH)\nprint(\"Dirty terms loaded:\", len(dirty_words))\nprint(dirty_words[:10])\n\ndef dirty_matches(term: str, dirty_list: list[str]) -> list[str]:\n    t = safe_str(term).lower()\n    hits = []\n    for w in dirty_list:\n        # phrase match, case-insensitive\n        if w in t:\n            hits.append(w)\n    return hits\n\nsearch_df[\"dirty_hits\"] = search_df[\"search_term\"].apply(lambda t: dirty_matches(t, dirty_words))\nsearch_df[\"is_dirty\"] = search_df[\"dirty_hits\"].apply(lambda x: len(x) > 0)\n\ndisplay(search_df[[\"search_engine\",\"search_term\",\"is_dirty\",\"dirty_hits\"]].head(25))\nprint(\"Dirty search events:\", int(search_df[\"is_dirty\"].sum()))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# -----------------------------\n# Interactive UI\n# -----------------------------\nengine_dd = widgets.Dropdown(\n    options=[(\"All\", \"\")] + [(e, e) for e in sorted(search_df[\"search_engine\"].unique())],\n    value=\"\",\n    description=\"Engine:\"\n)\n\nuser_txt = widgets.Text(\n    value=\"\",\n    placeholder=\"Filter by user contains (e.g., karen@)\",\n    description=\"User:\"\n)\n\nterm_txt = widgets.Text(\n    value=\"\",\n    placeholder=\"Search terms contains (e.g., bypass, zia, test123)\",\n    description=\"Term:\"\n)\n\nonly_dirty_cb = widgets.Checkbox(value=False, description=\"Only dirty matches\", indent=False)\n\ntopn = widgets.IntSlider(value=50, min=10, max=500, step=10, description=\"Rows:\", continuous_update=False)\n\nout = widgets.Output()\n\ncontrols = widgets.VBox([\n    widgets.HBox([engine_dd, only_dirty_cb, topn]),\n    widgets.HBox([user_txt, term_txt]),\n])\n\ndisplay(controls, out)\n\ndef _filter():\n    d = search_df\n    if engine_dd.value:\n        d = d[d[\"search_engine\"] == engine_dd.value]\n    if only_dirty_cb.value:\n        d = d[d[\"is_dirty\"]]\n    if user_txt.value.strip():\n        q = user_txt.value.strip().lower()\n        d = d[d[\"User\"].fillna(\"\").str.lower().str.contains(q, na=False)]\n    if term_txt.value.strip():\n        q = term_txt.value.strip().lower()\n        d = d[d[\"search_term\"].fillna(\"\").str.lower().str.contains(q, na=False)]\n    return d\n\ndef _style(d: pd.DataFrame):\n    cols = [\"Logged Time\",\"User\",\"search_engine\",\"search_term\",\"dirty_hits\",\"URL\",\"Policy Action\",\"Request Method\",\"Response Code\",\"Total Bytes\"]\n    cols = [c for c in cols if c in d.columns]\n    view = d[cols].copy()\n\n    def row_style(row):\n        styles = [\"\"] * len(view.columns)\n        if bool(row.get(\"is_dirty\", False)):\n            styles = [\"background-color: #ffe6e6;\"] * len(view.columns)  # light red\n            # add a darker left border on the term cell\n            if \"search_term\" in view.columns:\n                idx = list(view.columns).index(\"search_term\")\n                styles[idx] += \"border-left: 6px solid #dc3545;\"\n        return styles\n\n    return view.style.apply(row_style, axis=1).set_properties(**{\"font-size\": \"12px\"})\n\ndef render():\n    with out:\n        out.clear_output()\n        d = _filter()\n\n        # Summary cards\n        total = len(d)\n        dirty = int(d[\"is_dirty\"].sum())\n        uniq_terms = d[\"search_term\"].nunique(dropna=True)\n        display(HTML(f'''\n        <div style=\"display:flex; gap:16px; flex-wrap:wrap; margin:6px 0 10px 0;\">\n          <div style=\"padding:10px 12px; border:1px solid #ddd; border-radius:12px;\">\n            <div style=\"font-size:12px; color:#666;\">Filtered searches</div>\n            <div style=\"font-size:22px; font-weight:700;\">{total:,}</div>\n          </div>\n          <div style=\"padding:10px 12px; border:1px solid #ddd; border-radius:12px;\">\n            <div style=\"font-size:12px; color:#666;\">Unique terms</div>\n            <div style=\"font-size:22px; font-weight:700;\">{uniq_terms:,}</div>\n          </div>\n          <div style=\"padding:10px 12px; border:1px solid #ddd; border-radius:12px;\">\n            <div style=\"font-size:12px; color:#666;\">Dirty hits</div>\n            <div style=\"font-size:22px; font-weight:700;\">{dirty:,}</div>\n          </div>\n        </div>\n        '''))\n\n        # Charts\n        if total > 0:\n            vc = d[\"search_engine\"].value_counts().reset_index()\n            vc.columns = [\"search_engine\",\"count\"]\n            fig = px.bar(vc, x=\"count\", y=\"search_engine\", orientation=\"h\", title=\"Search engine volume (filtered)\")\n            fig.update_layout(height=280, margin=dict(l=10,r=10,t=45,b=10))\n            display(fig)\n\n            # Top terms (stack count)\n            tc = d[\"search_term\"].value_counts().head(20).reset_index()\n            tc.columns = [\"search_term\",\"count\"]\n            fig2 = px.bar(tc, x=\"count\", y=\"search_term\", orientation=\"h\", title=\"Top search terms (filtered, top 20)\")\n            fig2.update_layout(height=420, margin=dict(l=10,r=10,t=45,b=10))\n            display(fig2)\n\n        # Table\n        show = d.sort_values([\"is_dirty\",\"Logged Time\"], ascending=[False, True]).head(int(topn.value))\n        display(_style(show))\n\nfor w in [engine_dd, only_dirty_cb, topn, user_txt, term_txt]:\n    w.observe(lambda change: render(), names=\"value\")\n\nrender()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Optional: export the extracted searches for sharing / offline review\nOUT_CSV = \"../data/extracted_search_terms.csv\"\ncols = [\"Logged Time\",\"Event Time\",\"User\",\"search_engine\",\"search_term\",\"is_dirty\",\"dirty_hits\",\"URL\",\"Policy Action\",\"Request Method\",\"Response Code\",\"Total Bytes\",\"url_host\",\"domain\",\"tld\"]\ncols = [c for c in cols if c in search_df.columns]\nsearch_df[cols].to_csv(OUT_CSV, index=False)\nprint(\"Wrote:\", OUT_CSV)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}